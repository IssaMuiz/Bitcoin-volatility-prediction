# Bitcoin Volatility Prediction

## Project Overview

This project aims to predict the volatility of Bitcoin using historical price data and various machine learning techniques. The goal is to develop a model that can accurately forecast future volatility, which can be valuable for traders, investors, and risk managers in the cryptocurrency market.

## Project Initial Features

- Start – Start date of trading period

- End – End date of trading period

- Open – Opening price

- High – Highest price of the day

- Low – Lowest price of the day

- Close – Closing price

- Volume – Daily trading volume

- Market Cap – Market capitalization

## Project Structure

- `data/`: Contains the raw and processed datasets.
- `notebooks/`: Jupyter notebooks for data exploration, modeling, and analysis.
- `src/`: Source code for data preprocessing, model training, and evaluation.

## Project Problem Statement

The cryptocurrency market is known for its high volatility, which can lead to significant financial risks for traders and investors. Accurately predicting Bitcoin's volatility can help market participants make informed decisions, manage risk, and optimize their trading strategies. This project seeks to develop a machine learning model that can forecast Bitcoin's volatility based on historical price data and other relevant features.

## Project Success Criteria

- The model achieves a mean absolute error (MAE) of less than 0.05 on the test set.
- The model demonstrates consistent performance across different time periods and market conditions.

## Evaluation Metrics

- MAE
- RMSE
- R2_score

## Dataset

The dataset used in this project consists of historical Bitcoin price data, including features such as opening price, closing price, high, low, and trading volume. The data is sourced from [https://www.kaggle.com/datasets/priyamchoksi/bitcoin-historical-prices-and-activity-2010-] and covers a period from July 27, 2010, to May 22, 2024.

## Current Progress

- Data collection and loading completed.

- Initial data exploration and visualization performed.

- Datetime conversion and basic data cleaning steps implemented.

- Features like log_return, volatility, and range calculated and added to the dataset.

1.  Log returns were calculated from the closing prices. Instead of working directly with raw prices, log returns measure the percentage change from one day to the next. This transformation helps stabilize the data, reduce skewness, and make price movements more consistent over time. Financial markets tend to grow exponentially, so using returns instead of raw prices allows the model to focus on relative movement rather than absolute price levels.

2.  Volatility derived from log returns. Volatility represents the level of risk or uncertainty in the market. By predicting future volatility rather than price direction, the model focuses on estimating market instability, which is highly relevant for risk management and trading strategies.

3.  A range-based feature, often referred to as the daily range of motion, was also included. This measures the spread between the daily high and low prices. The range captures intraday price fluctuation and provides additional information about market activity and uncertainty within a single trading period. Larger ranges often indicate higher market tension or increased trading activity, which can signal rising volatility.

- After adding these features we have some missing values in the dataset, we have handled those missing values by dropping the rows with missing values.

- Exploratory Data Analysis (EDA) Summary
  - Exploratory data analysis was conducted to understand the structure, behavior, and relationships within the dataset. The key findings are as follows:
  1. The OHLC (Open, High, Low, Close) features exhibit strong positive correlations, indicating high multicollinearity. Most features are skewed, while the log returns approximate a normal distribution.
  2. Bitcoin prices remained relatively stable from inception until early 2017, after which a sustained upward trend emerged.
  3. Bitcoin volatility was extremely high in the early years, characterized by pronounced cycles of rising and falling risk. However, lower-risk periods became more frequent from 2015 onward, indicating a gradual stabilization of market behavior.

- Data Splitting (Time-Series Based Approach)

Following data cleaning, the dataset comprises 5,060 observations and 18 features.
Given the time-dependent nature of financial data, a chronological time-based splitting strategy was adopted to ensure methodological integrity and to prevent data leakage.
Unlike conventional random sampling, the dataset was partitioned sequentially to preserve temporal order and realistically simulate forecasting conditions.

- The data was divided as follows:

1.  Training Set (3036 observations)
    This subset contains the earliest portion of the dataset and is used to train the model by learning historical patterns and volatility dynamics.

2.  Validation Set (1,012 observations)
    This subset contains subsequent chronological observations and is used for model tuning and intermediate performance evaluation without exposing the model to future data.

3.  Test Set (1,012 observations)
    This subset contains the most recent observations and is reserved for final model evaluation. It assesses the model’s ability to generalize to unseen future data under realistic forecasting conditions.

- This chronological partitioning ensures that the model is trained exclusively on past information and evaluated on future observations, thereby aligning with best practices in time-series forecasting and financial modeling.

- In addition to the original price variables and the features added earlier, several engineered features were created to better capture the time-dependent behavior of Bitcoin’s market dynamics.
  - First, rolling statistical features were introduced. These include rolling averages and rolling standard deviations computed over fixed time windows. Rolling statistics allow the model to understand recent market conditions rather than relying on isolated daily observations. For example, a rolling average captures short-term trends, while a rolling standard deviation reflects recent variability in the market. These features help detect changes in market regimes, such as transitions from stable periods to highly volatile periods.

  - Next, Lag features were also added to incorporate historical memory into the model. Financial time series are not independent across days; what happens today is often influenced by what happened yesterday or last week. By including lagged versions of key variables such as volatility and returns, the model is able to learn temporal dependencies and persistence patterns. This is particularly important because volatility tends to cluster — periods of high volatility are often followed by continued high volatility.

  - Log transformation converts price movement into relative percentage changes. This helps stabilize the data, reduce skewness caused by exponential price growth, and make the time series more suitable for modeling. Financial markets typically exhibit compounding behavior, and log returns allow the model to focus on proportional movement rather than absolute price differences.

  - Finally, the prediction target was defined as future volatility by applying a shift operation to the computed volatility series. Instead of predicting the current day’s volatility, the model is trained to predict the next period’s volatility. This approach ensures that only past information is used to forecast future risk, aligning the modeling process with real-world forecasting conditions and preventing data leakage.

- Together, these engineered features transform raw market data into structured signals that reflect trend behavior, market memory, and risk dynamics. This feature design aligns with established practices in financial time-series modeling and improves the model’s ability to capture meaningful patterns in Bitcoin’s price behavior.

## Testing

## Model Packaging and Deployment

## Stack & Tools

- Python
- jupyter Notebooks
- Pandas
- NumPy
- Matplotlib & Seaborn
- Scikit-learn
- statsmodels
- Git & GitHub
- joblib
- pytest

## Project Steps

- Data collection and loading
- Data cleaning and preprocessing
- Exploratory data analysis (EDA)
- Data splitting
- Baseline modeling
- Error analysing
- Build full preprocessing + model pipeline
- Feature engineering
- Artifact creation
- Model training and tuning
- Final evaluation (test set)
- Unit testing
- Model packaging and deployment readiness
- Monitoring and maintenance plan

## Author

Issa Muiz
Machine Learning Engineer
